{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9kexyeXG_3u"
      },
      "source": [
        "# CMSC848F Assignment 1\n",
        "## Name: Hritvik Choudhari\n",
        "## UID: 119208793"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkbnhmCuHPnE"
      },
      "source": [
        "### Importing necessary libraries and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvX683sky8Ur"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith((\"1.13.\", \"2.0.\")) and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J4MsgvczEFN"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "upsQ_aQxzE3Y"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/assignment1-main/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv6uCeAo-VeO"
      },
      "outputs": [],
      "source": [
        "!pip install PyMCubes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX2R0u2P0Rha"
      },
      "source": [
        "# **0.1 Rendering your first mesh**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy8ODv9M4qqv",
        "outputId": "6b2aa0f9-1fc9-44c7-acb9-d29651526e0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-1f9d7ea423a0>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  vertices = torch.tensor(vertices, dtype=torch.float32)\n",
            "<ipython-input-13-1f9d7ea423a0>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  faces = torch.tensor(faces, dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "from starter.utils import get_device, get_mesh_renderer, load_cow_mesh\n",
        "import pytorch3d, torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the device (CPU or GPU)\n",
        "device = get_device()\n",
        "\n",
        "# Load cow mesh vertices and faces\n",
        "vertices, faces = load_cow_mesh('/content/drive/MyDrive/assignment1-main/data/cow.obj')\n",
        "\n",
        "def section0_1(vertices, faces, output_path, color, image_size):\n",
        "    # Get the renderer.\n",
        "    renderer = get_mesh_renderer(image_size=image_size)\n",
        "\n",
        "    # Convert vertices and faces to PyTorch tensors\n",
        "    vertices = torch.tensor(vertices, dtype=torch.float32)\n",
        "    faces = torch.tensor(faces, dtype=torch.int32)\n",
        "\n",
        "    # Add batch dimension (1) to vertices and faces\n",
        "    vertices = vertices.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
        "    faces = faces.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
        "\n",
        "    # Create a texture using the specified color\n",
        "    textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
        "    textures = textures * torch.tensor(color)  # (1, N_v, 3)\n",
        "\n",
        "    # Create a Meshes object with vertices, faces, and textures\n",
        "    mesh = pytorch3d.structures.Meshes(\n",
        "        verts=vertices,\n",
        "        faces=faces,\n",
        "        textures=pytorch3d.renderer.TexturesVertex(textures),\n",
        "    )\n",
        "    mesh = mesh.to(device)\n",
        "\n",
        "    # Define camera parameters (position, orientation, and field of view)\n",
        "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
        "        R=torch.eye(3).unsqueeze(0),  # Identity rotation matrix\n",
        "        T=torch.tensor([[0, 0, 3]]),  # Translation along the z-axis\n",
        "        fov=90,  # Field of view (in degrees)\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Define lighting parameters (position of point light source)\n",
        "    lights = pytorch3d.renderer.PointLights(location=[[-1, -1, -3]], device=device)\n",
        "\n",
        "    # Render the mesh using the specified camera and lights\n",
        "    rend = renderer(mesh, cameras=cameras, lights=lights)\n",
        "\n",
        "    # Extract the RGB channels and convert to a NumPy array\n",
        "    rend = rend.cpu().numpy()[0, ..., :3]  # (B, H, W, 4) -> (H, W, 3)\n",
        "\n",
        "    # Save the rendered image as a JPEG file\n",
        "    plt.imsave(output_path, rend)\n",
        "\n",
        "# Call the function to render and save the image\n",
        "section0_1(vertices, faces, '/content/drive/MyDrive/assignment1-main/section0_1_render.jpg', [0.3, 0.9, 0.5], 512)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rene_WrAhIih"
      },
      "source": [
        "# 1. Practicing with Cameras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhSSU8uUhYGI"
      },
      "source": [
        "## 1.1. 360-degree Renders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KOmfM8AnexY7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch3d\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from starter.utils import get_device, get_mesh_renderer, load_cow_mesh\n",
        "\n",
        "# Get the device (CPU or GPU)\n",
        "device = get_device()\n",
        "\n",
        "# Load cow mesh vertices and faces\n",
        "vertices, faces = load_cow_mesh('/content/drive/MyDrive/assignment1-main/data/cow.obj')\n",
        "\n",
        "# Add batch dimension to vertices and faces\n",
        "vertices = vertices.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
        "faces = faces.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
        "\n",
        "# Create a texture for the mesh\n",
        "textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
        "textures = textures * torch.tensor([0.5, 0.2, 0.1])  # (1, N_v, 3)\n",
        "\n",
        "# Create a Meshes object with vertices, faces, and textures\n",
        "mesh = pytorch3d.structures.Meshes(\n",
        "    verts=vertices,\n",
        "    faces=faces,\n",
        "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
        ")\n",
        "mesh = mesh.to(device)\n",
        "\n",
        "def section1_1(num_frames, mesh, image_size):\n",
        "    # Define the number of frames for the 360-degree rotation.\n",
        "    num_frames = num_frames\n",
        "    renderer = get_mesh_renderer(image_size=image_size)\n",
        "\n",
        "    # Generate a list of camera positions to cover a full rotation.\n",
        "    camera_rotation, camera_translation = pytorch3d.renderer.look_at_view_transform(\n",
        "        dist=-4.0, elev=0, azim=torch.linspace(0, 360, num_frames), device=device\n",
        "    )\n",
        "\n",
        "    # Create a list to store the rendered images.\n",
        "    rendered_images = []\n",
        "\n",
        "    # Render images for each camera position.\n",
        "    for camera_pose in zip(camera_rotation, camera_translation):\n",
        "        # Update the camera position.\n",
        "        R = camera_pose[0].unsqueeze(0)\n",
        "        T = camera_pose[1].unsqueeze(0)\n",
        "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
        "            R=R,\n",
        "            T=T,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        # Render the image.\n",
        "        rendered_image = renderer(mesh, cameras=cameras)\n",
        "        if rendered_image.shape[3] == 4:\n",
        "            rendered_image = rendered_image[:, :, :, :3]  # Keep only RGB channels\n",
        "\n",
        "        # Convert the rendered image to a numpy array and add it to the list.\n",
        "        rendered_image_rgb = (rendered_image.squeeze() * 255).byte().cpu().numpy()\n",
        "        rendered_images.append(rendered_image_rgb)\n",
        "\n",
        "    return rendered_images\n",
        "\n",
        "# Save the rendered images as frames for the gif.\n",
        "image_list = section1_1(36, mesh, 512)\n",
        "output_path = \"/content/drive/MyDrive/assignment1-main/section1_1.gif\"\n",
        "imageio.mimsave(output_path, image_list, duration=50, loop=0)  # Adjust the duration as needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBy2QsQH0-AJ"
      },
      "source": [
        "## 1.2 Re-creating the Dolly Zoom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "32705f6dd4ed4e709befb4c561c7c780",
            "77b88fb52c324eb6b926a3f0e82560dc",
            "7dffa80b53314e0a8c731e19dad6574f",
            "c7f548f8ead0463799985fe7d2f5a161",
            "c90d4fec0af74f6f82d950331ca23cad",
            "6b1adbe457954bfdaef7eee5c80f900a",
            "bd9fb3654fdf4589a1db21abd00fcad2",
            "d9db269b210544f9b526e33961a631f4",
            "f7c777a2ef8846bb9c27ce0c030e871d",
            "95a3bd58cf444ce0954386bc7812dda3",
            "2f466b2e04ec4aeaac68f2460cf6a45a"
          ]
        },
        "id": "rGEPIvx9nFrf",
        "outputId": "ff47c57e-8cee-46af-8148-eefac869dfbf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32705f6dd4ed4e709befb4c561c7c780",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import argparse\n",
        "import imageio\n",
        "import numpy as np\n",
        "import pytorch3d\n",
        "import torch\n",
        "from PIL import Image, ImageDraw\n",
        "from tqdm.auto import tqdm\n",
        "from starter.utils import get_device, get_mesh_renderer\n",
        "\n",
        "def section1_2(output_file,\n",
        "    image_size=256,\n",
        "    num_frames=10,\n",
        "    duration=100,\n",
        "    device=None\n",
        "):\n",
        "    # Check if a specific device is specified, otherwise get the default device.\n",
        "    if device is None:\n",
        "        device = get_device()\n",
        "\n",
        "    # Load the cow mesh.\n",
        "    mesh = pytorch3d.io.load_objs_as_meshes([\"/content/drive/MyDrive/assignment1-main/data/cow_on_plane.obj\"])\n",
        "    mesh = mesh.to(device)\n",
        "\n",
        "    # Create a mesh renderer.\n",
        "    renderer = get_mesh_renderer(image_size=image_size, device=device)\n",
        "\n",
        "    # Define point lights.\n",
        "    lights = pytorch3d.renderer.PointLights(location=[[0.0, 0.0, -3.0]], device=device)\n",
        "\n",
        "    # Create a sequence of field of view (FOV) values for zoom effect.\n",
        "    fovs = torch.linspace(0, 125, num_frames)\n",
        "\n",
        "    # List to store rendered images.\n",
        "    renders = []\n",
        "\n",
        "    # Iterate over FOV values and render frames.\n",
        "    for fov in tqdm(fovs):\n",
        "        # Calculate the distance based on FOV to achieve the dolly zoom effect.\n",
        "        distance = 2.5 / np.tan(np.radians(fov / 2))\n",
        "\n",
        "        # Set the camera translation to create dolly zoom effect.\n",
        "        T = [[0, 0, distance]]\n",
        "\n",
        "        # Create perspective cameras with the specified FOV and translation.\n",
        "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(fov=fov, T=T, device=device)\n",
        "\n",
        "        # Render the image.\n",
        "        rend = renderer(mesh, cameras=cameras, lights=lights)\n",
        "\n",
        "        # Extract the RGB channels.\n",
        "        rend = rend[0, ..., :3].cpu().numpy()\n",
        "        renders.append(rend)\n",
        "\n",
        "    # Create a list to store final images with text annotations.\n",
        "    images = []\n",
        "\n",
        "    # Iterate over rendered frames.\n",
        "    for i, r in enumerate(renders):\n",
        "        # Convert the numpy array to PIL Image.\n",
        "        image = Image.fromarray((r * 255).astype(np.uint8))\n",
        "\n",
        "        # Create a drawing context to add text annotations.\n",
        "        draw = ImageDraw.Draw(image)\n",
        "\n",
        "        # Add FOV information as text.\n",
        "        draw.text((20, 20), f\"fov: {fovs[i]:.2f}\", fill=(255, 0, 0))\n",
        "\n",
        "        # Append the annotated image to the list.\n",
        "        images.append(np.array(image))\n",
        "\n",
        "    # Save the list of images as a GIF.\n",
        "    imageio.mimsave(output_file, images, duration=duration, loop=0)\n",
        "\n",
        "# Call the dolly_zoom function with the specified parameters.\n",
        "section1_2(image_size=256,\n",
        "        num_frames=30,\n",
        "        duration=150,\n",
        "        output_file='/content/drive/MyDrive/assignment1-main/section1_2.gif'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qAOmu4EA33a"
      },
      "source": [
        "# 2. Practicing with Meshes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk6VLLjGA6eC"
      },
      "source": [
        "## 2.1 Constructing a Tetrahedron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lhFrlqtj3vYX"
      },
      "outputs": [],
      "source": [
        "from starter.utils import get_device, get_mesh_renderer, load_cow_mesh\n",
        "import pytorch3d, torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the device for computation (e.g., CUDA GPU or CPU).\n",
        "device = get_device()\n",
        "\n",
        "# Define the vertices of the mesh.\n",
        "vertices = torch.tensor([[\n",
        "    [0.0, 0.0, 0.0],   # Vertex 1\n",
        "    [1.0, 0.0, 0.0],   # Vertex 2\n",
        "    [0.5, 1.0, 0.0],   # Vertex 3\n",
        "    [0.5, 0.5, 1.0],   # Vertex 4\n",
        "]], dtype=torch.float32)\n",
        "\n",
        "# Define the faces of the mesh.\n",
        "faces = torch.tensor([[\n",
        "    [0, 1, 2],         # Face 1 (vertices 0, 1, 2)\n",
        "    [0, 1, 3],         # Face 2 (vertices 0, 1, 3)\n",
        "    [0, 2, 3],         # Face 3 (vertices 0, 2, 3)\n",
        "    [1, 2, 3],         # Face 4 (vertices 1, 2, 3)\n",
        "]], dtype=torch.int32)\n",
        "\n",
        "# Define the textures for the mesh.\n",
        "textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
        "textures = textures * torch.tensor([0.1, 0.7, 0.9])  # (1, N_v, 3)\n",
        "\n",
        "# Create a Meshes object with vertices, faces, and textures.\n",
        "mesh = pytorch3d.structures.Meshes(\n",
        "    verts=vertices,\n",
        "    faces=faces,\n",
        "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
        ")\n",
        "\n",
        "# Move the mesh to the specified device (e.g., GPU).\n",
        "mesh = mesh.to(device)\n",
        "\n",
        "# Define a function to render images from different camera angles.\n",
        "def section2_1(mesh, num_frames, image_size):\n",
        "    num_frames = num_frames\n",
        "    renderer = get_mesh_renderer(image_size=image_size)\n",
        "\n",
        "    # Generate a list of camera positions to cover a full rotation.\n",
        "    camera_rotation, camera_translation = pytorch3d.renderer.look_at_view_transform(\n",
        "        dist= -4.0, elev=0, azim=torch.linspace(0, 360, num_frames), device=device\n",
        "    )\n",
        "\n",
        "    # Create a list to store the rendered images.\n",
        "    rendered_images = []\n",
        "\n",
        "    # Render images for each camera position.\n",
        "    for camera_pose in zip(camera_rotation, camera_translation):\n",
        "        # Update the camera position.\n",
        "        R = camera_pose[0].unsqueeze(0)\n",
        "        T = camera_pose[1].unsqueeze(0)\n",
        "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
        "            R=R,\n",
        "            T=T,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        # Render the image.\n",
        "        rendered_image = renderer(mesh, cameras=cameras)\n",
        "\n",
        "        # Keep only the RGB channels if there's an alpha channel.\n",
        "        if rendered_image.shape[3] == 4:\n",
        "            rendered_image = rendered_image[:, :, :, :3]\n",
        "\n",
        "        # Convert the rendered image to a numpy array and add it to the list.\n",
        "        rendered_image_rgb = (rendered_image.squeeze() * 255).byte().cpu().numpy()\n",
        "        rendered_images.append(rendered_image_rgb)\n",
        "\n",
        "    return rendered_images\n",
        "\n",
        "# Save the rendered images as frames for the gif.\n",
        "image_list = section2_1(mesh, num_frames=36, image_size=512)\n",
        "output_path = \"/content/drive/MyDrive/assignment1-main/section2_1.gif\"\n",
        "imageio.mimsave(output_path, image_list, duration=50, loop=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rimJ7EOKCCR"
      },
      "source": [
        "## 2.2 Constructing a Cube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pr4lvSgYEp_Y"
      },
      "outputs": [],
      "source": [
        "from starter.utils import get_device, get_mesh_renderer, load_cow_mesh\n",
        "import pytorch3d, torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the device for computation (e.g., CUDA GPU or CPU).\n",
        "device = get_device()\n",
        "\n",
        "# Define the vertices of a cube.\n",
        "vertices = torch.tensor([[\n",
        "    [-1, -1, -1],  # Vertex 1\n",
        "    [1, -1, -1],   # Vertex 2\n",
        "    [1, 1, -1],    # Vertex 3\n",
        "    [-1, 1, -1],   # Vertex 4\n",
        "    [-1, -1, 1],   # Vertex 5\n",
        "    [1, -1, 1],    # Vertex 6\n",
        "    [1, 1, 1],     # Vertex 7\n",
        "    [-1, 1, 1],    # Vertex 8\n",
        "]], dtype=torch.float32)\n",
        "\n",
        "# Define the faces of the cube using two sets of triangles.\n",
        "faces = torch.tensor([[\n",
        "    [0, 1, 2], [0, 2, 3],  # Bottom face (vertices 0, 1, 2 and 0, 2, 3)\n",
        "    [4, 5, 6], [4, 6, 7],  # Top face (vertices 4, 5, 6 and 4, 6, 7)\n",
        "    [0, 1, 5], [0, 5, 4],  # Front face (vertices 0, 1, 5 and 0, 5, 4)\n",
        "    [2, 3, 7], [2, 7, 6],  # Back face (vertices 2, 3, 7 and 2, 7, 6)\n",
        "    [1, 2, 6], [1, 6, 5],  # Right face (vertices 1, 2, 6 and 1, 6, 5)\n",
        "    [0, 3, 7], [0, 7, 4],  # Left face (vertices 0, 3, 7 and 0, 7, 4)\n",
        "]], dtype=torch.int32)\n",
        "\n",
        "# Define the textures for the cube.\n",
        "textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
        "textures = textures * torch.tensor([0.1, 0.7, 0.9])  # (1, N_v, 3)\n",
        "\n",
        "# Create a Meshes object with vertices, faces, and textures.\n",
        "mesh = pytorch3d.structures.Meshes(\n",
        "    verts=vertices,\n",
        "    faces=faces,\n",
        "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
        ")\n",
        "\n",
        "# Move the mesh to the specified device (e.g., GPU).\n",
        "mesh = mesh.to(device)\n",
        "\n",
        "# Define a function to render images from different camera angles.\n",
        "def section2_2(mesh, num_frames, image_size):\n",
        "    num_frames = num_frames\n",
        "    renderer = get_mesh_renderer(image_size=image_size)\n",
        "\n",
        "    # Generate a list of camera positions to cover a full rotation.\n",
        "    camera_rotation, camera_translation = pytorch3d.renderer.look_at_view_transform(\n",
        "        dist=-5.0, elev=0, azim=torch.linspace(0, 360, num_frames), device=device\n",
        "    )\n",
        "\n",
        "    # Create a list to store the rendered images.\n",
        "    rendered_images = []\n",
        "\n",
        "    # Render images for each camera position.\n",
        "    for camera_pose in zip(camera_rotation, camera_translation):\n",
        "        # Update the camera position.\n",
        "        R = camera_pose[0].unsqueeze(0)\n",
        "        T = camera_pose[1].unsqueeze(0)\n",
        "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
        "            R=R,\n",
        "            T=T,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        # Render the image.\n",
        "        rendered_image = renderer(mesh, cameras=cameras)\n",
        "\n",
        "        # Keep only the RGB channels if there's an alpha channel.\n",
        "        if rendered_image.shape[3] == 4:\n",
        "            rendered_image = rendered_image[:, :, :, :3]\n",
        "\n",
        "        # Convert the rendered image to a numpy array and add it to the list.\n",
        "        rendered_image_rgb = (rendered_image.squeeze() * 255).byte().cpu().numpy()\n",
        "        rendered_images.append(rendered_image_rgb)\n",
        "\n",
        "    return rendered_images\n",
        "\n",
        "# Save the rendered images as frames for the gif.\n",
        "image_list = section2_2(mesh, num_frames=36, image_size=512)\n",
        "output_path = \"/content/drive/MyDrive/assignment1-main/section2_2.gif\"\n",
        "imageio.mimsave(output_path, image_list, duration=50, loop=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBw1KLsbLk2I"
      },
      "source": [
        "# 3. Re-texturing a mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wy9gQFqoKZPz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch3d\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from starter.utils import get_device, get_mesh_renderer, load_cow_mesh\n",
        "\n",
        "# Get the device for computation (e.g., CUDA GPU or CPU).\n",
        "device = get_device()\n",
        "\n",
        "# Load the cow mesh vertices and faces from a file.\n",
        "vertices, faces = load_cow_mesh('/content/drive/MyDrive/assignment1-main/data/cow.obj')\n",
        "\n",
        "# Move vertices and faces to the specified device and add batch dimensions.\n",
        "vertices = vertices.unsqueeze(0).to(device)  # (N_v, 3) -> (1, N_v, 3)\n",
        "faces = faces.unsqueeze(0).to(device)  # (N_f, 3) -> (1, N_f, 3)\n",
        "\n",
        "# Calculate the minimum and maximum z-coordinates in the mesh.\n",
        "z_min = vertices[:, :, 2].min()\n",
        "z_max = vertices[:, :, 2].max()\n",
        "\n",
        "# Define two colors for the front and back faces.\n",
        "color1 = torch.tensor([0, 1, 1], dtype=torch.float32, device=device)\n",
        "color2 = torch.tensor([1, 0.2, 0.5], dtype=torch.float32, device=device)\n",
        "\n",
        "# Compute interpolated colors based on vertex depth.\n",
        "alphas = (vertices[:, :, 2] - z_min) / (z_max - z_min)\n",
        "colors = (alphas.unsqueeze(-1) * color2 + (1 - alphas.unsqueeze(-1)) * color1)\n",
        "\n",
        "# Create a Meshes object with vertices, faces, and vertex colors.\n",
        "mesh = pytorch3d.structures.Meshes(\n",
        "    verts=vertices,\n",
        "    faces=faces,\n",
        "    textures=pytorch3d.renderer.TexturesVertex(colors),\n",
        ")\n",
        "mesh = mesh.to(device)\n",
        "\n",
        "# Define a function to render images from different camera angles.\n",
        "def section3(num_frames, mesh, image_size):\n",
        "    # Define the number of frames for the 360-degree rotation.\n",
        "    num_frames = num_frames\n",
        "    renderer = get_mesh_renderer(image_size=image_size)\n",
        "\n",
        "    # Generate a list of camera positions to cover a full rotation.\n",
        "    camera_rotation, camera_translation = pytorch3d.renderer.look_at_view_transform(\n",
        "        dist= -4.0, elev=0, azim=torch.linspace(0, 360, num_frames), device=device\n",
        "    )\n",
        "\n",
        "    # Create a list to store the rendered images.\n",
        "    rendered_images = []\n",
        "\n",
        "    # Render images for each camera position.\n",
        "    for camera_pose in zip(camera_rotation, camera_translation):\n",
        "        # Update the camera position.\n",
        "        R = camera_pose[0].unsqueeze(0)\n",
        "        T = camera_pose[1].unsqueeze(0)\n",
        "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
        "            R=R,\n",
        "            T=T,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        # Render the image.\n",
        "        rendered_image = renderer(mesh, cameras=cameras)\n",
        "\n",
        "        # Keep only the RGB channels if there's an alpha channel.\n",
        "        if rendered_image.shape[3] == 4:\n",
        "            rendered_image = rendered_image[:, :, :, :3]\n",
        "\n",
        "        # Convert the rendered image to a numpy array and add it to the list.\n",
        "        rendered_image_rgb = (rendered_image.squeeze() * 255).byte().cpu().numpy()\n",
        "        rendered_images.append(rendered_image_rgb)\n",
        "\n",
        "    return rendered_images\n",
        "\n",
        "# Save the rendered images as frames for the gif.\n",
        "image_list = section3(36, mesh, 512)\n",
        "output_path = \"/content/drive/MyDrive/assignment1-main/section3.gif\"\n",
        "imageio.mimsave(output_path, image_list, duration=100, loop=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84zXofjxjMkr"
      },
      "source": [
        "# 4. Camera Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cJ6KXyV4f_XB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pytorch3d\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from starter.utils import get_device, get_mesh_renderer\n",
        "\n",
        "# Function to render the cow with different poses.\n",
        "def render_cow(R_relative, T_relative, cow_path=\"data/cow_with_axis.obj\", image_size=256, device=None):\n",
        "    if device is None:\n",
        "        device = get_device()\n",
        "\n",
        "    # Load the cow mesh as Meshes object.\n",
        "    meshes = pytorch3d.io.load_objs_as_meshes([cow_path]).to(device)\n",
        "\n",
        "    # Calculate the updated rotation and translation matrices for the camera.\n",
        "    R = R_relative @ torch.tensor([[1.0, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "    T = R_relative @ torch.tensor([0.0, 0, 3]) + T_relative\n",
        "\n",
        "    # Create a mesh renderer and define cameras and lights.\n",
        "    renderer = get_mesh_renderer(image_size=image_size)\n",
        "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
        "        R=R.t().unsqueeze(0), T=T.unsqueeze(0), device=device,\n",
        "    )\n",
        "    lights = pytorch3d.renderer.PointLights(location=[[0, 0.0, -3.0]], device=device)\n",
        "\n",
        "    # Render the image.\n",
        "    rend = renderer(meshes, cameras=cameras, lights=lights)\n",
        "\n",
        "    # Return the rendered image.\n",
        "    return rend[0, ..., :3].cpu().numpy()\n",
        "\n",
        "# Define different poses for the cow.\n",
        "pose1 = (\n",
        "    torch.tensor([\n",
        "        [torch.cos(torch.tensor(90.0).to(torch.float32) * (torch.pi / 180)), -torch.sin(torch.tensor(90.0).to(torch.float32) * (torch.pi / 180)), 0],\n",
        "        [torch.sin(torch.tensor(90.0).to(torch.float32) * (torch.pi / 180)), torch.cos(torch.tensor(90.0).to(torch.float32) * (torch.pi / 180)), 0],\n",
        "        [0, 0, 1]\n",
        "    ]).to(torch.float32),\n",
        "    [0, 0, 0]\n",
        ")\n",
        "\n",
        "pose2 = (\n",
        "    torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]]).to(torch.float32),\n",
        "    [0, 0, 3]\n",
        ")\n",
        "\n",
        "pose3 = (\n",
        "    torch.tensor([\n",
        "        [torch.cos(torch.tensor(10.0).to(torch.float32) * (torch.pi / 180)), 0, -torch.sin(torch.tensor(10.0).to(torch.float32) * (torch.pi / 180))],\n",
        "        [0, 1, 0],\n",
        "        [torch.sin(torch.tensor(10.0).to(torch.float32) * (torch.pi / 180)), 0, torch.cos(torch.tensor(10.0).to(torch.float32) * (torch.pi / 180))]\n",
        "    ]).to(torch.float32),\n",
        "    [0.75, -0.25, -0.45]\n",
        ")\n",
        "\n",
        "pose4 = (\n",
        "    torch.tensor([\n",
        "        [torch.cos(torch.tensor(90.0).to(torch.float32) * (torch.pi / 180)), 0, -torch.sin(torch.tensor(90.0).to(torch.float32) * (torch.pi / 180))],\n",
        "        [0, 1, 0],\n",
        "        [torch.sin(torch.tensor(90.0).to(torch.float32) * (torch.pi / 180)), 0, torch.cos(torch.tensor(90.0).to(torch.float32) * (torch.pi / 180))]\n",
        "    ]).to(torch.float32),\n",
        "    [3, 0, 3]\n",
        ")\n",
        "\n",
        "# Create a list of poses.\n",
        "poses = [pose1, pose2, pose3, pose4]\n",
        "\n",
        "# Render and save images for each pose.\n",
        "for i, val in enumerate(poses):\n",
        "    plt.imsave(\"/content/drive/MyDrive/assignment1-main/section4_{}.jpg\".format(i+1), render_cow(cow_path=\"/content/drive/MyDrive/assignment1-main/data/cow_with_axis.obj\", image_size=256, R_relative=val[0], T_relative=torch.tensor(val[1])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Mhl1Ym5XH6"
      },
      "source": [
        "# 5. Rendering Generic 3D Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMiUCfgXVAA2"
      },
      "source": [
        "## 5.1 Rendering Point Clouds from RGB-D Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CXYBQA7da1ew"
      },
      "outputs": [],
      "source": [
        "from starter.utils import get_points_renderer, get_device, unproject_depth_image\n",
        "from starter.render_generic import load_rgbd_data\n",
        "import pytorch3d\n",
        "import imageio\n",
        "\n",
        "# Get the device (CPU or GPU) for PyTorch operations.\n",
        "device = get_device()\n",
        "\n",
        "# Load the RGBD data from a pickle file.\n",
        "data = load_rgbd_data(path=\"/content/drive/MyDrive/assignment1-main/data/rgbd_data.pkl\")\n",
        "\n",
        "# Separate data for the first and second images.\n",
        "data_img1 = [data['rgb1'], data['mask1'], data['depth1'], data['cameras1']]\n",
        "data_img2 = [data['rgb2'], data['mask2'], data['depth2'], data['cameras2']]\n",
        "\n",
        "# Unproject the depth image to get point clouds and RGB values for both images.\n",
        "pc1, rgb1 = unproject_depth_image(torch.Tensor(data_img1[0]), torch.Tensor(data_img1[1]), torch.Tensor(data_img1[2]), data_img1[3])\n",
        "pc2, rgb2 = unproject_depth_image(torch.Tensor(data_img2[0]), torch.Tensor(data_img2[1]), torch.Tensor(data_img2[2]), data_img2[3])\n",
        "\n",
        "# Combine the point clouds and RGB values from both images.\n",
        "combined_points = torch.cat([pc1, pc2], dim=0)\n",
        "combined_features = torch.cat([rgb1, rgb2], dim=0)\n",
        "\n",
        "# Define a function to render the combined point cloud with rotation.\n",
        "def section5_1(pc, rgb, rotation_matrix, device):\n",
        "    # Prepare the points and RGB data and apply the rotation matrix.\n",
        "    points = pc.unsqueeze(0)  # 1 x N x 3\n",
        "    rgb = rgb.unsqueeze(0)    # 1 x N x 3\n",
        "    points = torch.matmul(points, rotation_matrix.T)\n",
        "\n",
        "    # Create a Pointclouds object.\n",
        "    point_cloud = pytorch3d.structures.Pointclouds(\n",
        "        points=points, features=rgb\n",
        "    ).to(device)\n",
        "\n",
        "    # Get the renderer for points.\n",
        "    points_renderer = get_points_renderer(\n",
        "        image_size=256,\n",
        "        radius=0.01,\n",
        "    )\n",
        "\n",
        "    # Generate camera positions for rendering.\n",
        "    camera_rotation, camera_translation = pytorch3d.renderer.look_at_view_transform(\n",
        "        dist=7.0, elev=0.0, azim=torch.linspace(0, 360, 36), device=device\n",
        "    )\n",
        "\n",
        "    # Create a list to store the rendered images.\n",
        "    rendered_images = []\n",
        "\n",
        "    # Render images for each camera position.\n",
        "    for camera_pose in zip(camera_rotation, camera_translation):\n",
        "        # Update the camera position.\n",
        "        R = camera_pose[0].unsqueeze(0)\n",
        "        T = camera_pose[1].unsqueeze(0)\n",
        "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
        "            R=R,\n",
        "            T=T,\n",
        "            device=device,\n",
        "        )\n",
        "        rendered_image = points_renderer(point_cloud, cameras=cameras)\n",
        "\n",
        "        # Keep only RGB channels (remove alpha if exists).\n",
        "        if rendered_image.shape[3] == 4:\n",
        "            rendered_image = rendered_image[:, :, :, :3]\n",
        "\n",
        "        # Convert the rendered image to a numpy array and add it to the list.\n",
        "        rendered_image_rgb = (rendered_image.squeeze() * 255).byte().cpu().numpy()\n",
        "        rendered_images.append(rendered_image_rgb)\n",
        "\n",
        "    return rendered_images\n",
        "\n",
        "# Define a rotation matrix for the point cloud.\n",
        "rotation_matrix = torch.tensor([\n",
        "    [-1, 0, 0],\n",
        "    [0, -1, 0],\n",
        "    [0, 0, 1]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Render the point cloud with rotation and save it as a GIF.\n",
        "image_list = section5_1(pc2, rgb2, rotation_matrix, device)\n",
        "output_path = \"/content/drive/MyDrive/assignment1-main/section5_1_2.gif\"\n",
        "imageio.mimsave(output_path, image_list, duration=100, loop=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50B_2K568MA3"
      },
      "source": [
        "## 5.2 Parametric Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqmcIA9scBMq",
        "outputId": "731dc778-c729-4899-d3e7-27e767b8fb98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pytorch3d\n",
        "import torch\n",
        "from torch import cos, sin\n",
        "import numpy as np\n",
        "import imageio\n",
        "from starter.utils import get_points_renderer, get_device\n",
        "\n",
        "def section5_2(azimuth, R=0.4, r=0.2, image_size=256, num_samples= 150, device=None):\n",
        "    \"\"\"\n",
        "    Renders a torus using parametric sampling. Samples num_samples ** 2 points.\n",
        "    \"\"\"\n",
        "\n",
        "    if device is None:\n",
        "        device = get_device()\n",
        "\n",
        "    # Generate phi and theta values for sampling.\n",
        "    phi_vals = torch.linspace(0, 2 * np.pi, num_samples)\n",
        "    theta_vals = torch.linspace(0, 2 * np.pi, num_samples)\n",
        "\n",
        "    # Create a grid of phi and theta values.\n",
        "    phi_mesh, theta_mesh = torch.meshgrid(phi_vals, theta_vals)\n",
        "\n",
        "    # Calculate x, y, and z coordinates of points on the torus.\n",
        "    x = (R + r * cos(phi_mesh)) * cos(theta_mesh)\n",
        "    y = (R + r * cos(phi_mesh)) * sin(theta_mesh)\n",
        "    z = r * sin(phi_mesh)\n",
        "\n",
        "    # Stack the coordinates to create the point cloud.\n",
        "    points = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)\n",
        "\n",
        "    # Normalize the color based on the point cloud coordinates.\n",
        "    color = (points - points.min()) / (points.max() - points.min())\n",
        "\n",
        "    # Create a Pointclouds object with points and colors.\n",
        "    torus_pc = pytorch3d.structures.Pointclouds(\n",
        "        points=[points], features=[color],\n",
        "    ).to(device)\n",
        "\n",
        "    # Define the view transformation using azimuth angle.\n",
        "    R, T = pytorch3d.renderer.look_at_view_transform(2, 3, azimuth, degrees=False)\n",
        "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
        "\n",
        "    # Get the points renderer and render the torus.\n",
        "    renderer = get_points_renderer(image_size=image_size, device=device)\n",
        "    rend = renderer(torus_pc, cameras=cameras)\n",
        "    rend = rend[0, ..., :3]  # (B, H, W, 4) -> (H, W, 3)\n",
        "\n",
        "    return (rend.squeeze() * 255).byte().cpu().numpy()\n",
        "\n",
        "# Define a range of azimuth angles to create a 360-degree view.\n",
        "azimuth = torch.linspace(0, 2 * np.pi, 36)\n",
        "torus_views = []\n",
        "\n",
        "# Render the torus from different viewpoints and collect the images.\n",
        "for i in azimuth:\n",
        "    view = section5_2(i)\n",
        "    torus_views.append(view)\n",
        "\n",
        "# Generate a GIF for the 360-degree view.\n",
        "imageio.mimsave('/content/drive/MyDrive/assignment1-main/section5_2.gif', torus_views, duration=70, loop=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMl64seWF6yA"
      },
      "source": [
        "## 5.3 Implicit Surfaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OU0RtWTXf3zB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pytorch3d\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "import mcubes\n",
        "from starter.utils import get_mesh_renderer, get_device\n",
        "\n",
        "def section5_3(azimuth, R=0.4, r=0.2, image_size=256, num_samples= 200, device=None):\n",
        "    \"\"\"\n",
        "    Renders a torus using parametric sampling. Samples num_samples ** 2 points.\n",
        "\n",
        "    Parameters:\n",
        "    - azimuth (float): The azimuth angle for the viewpoint.\n",
        "    - R (float): Major radius of the torus.\n",
        "    - r (float): Minor radius of the torus.\n",
        "    - image_size (int): Size of the rendered image.\n",
        "    - num_samples (int): Number of samples for parametric sampling.\n",
        "    - device (str): Device (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: Rendered image as a numpy array.\n",
        "    \"\"\"\n",
        "\n",
        "    if device is None:\n",
        "        device = get_device()\n",
        "\n",
        "    # Define voxel grid parameters.\n",
        "    voxel_size = 64\n",
        "    min_value = -1.1\n",
        "    max_value = 1.1\n",
        "\n",
        "    # Create a 3D grid for voxels.\n",
        "    X, Y, Z = torch.meshgrid([torch.linspace(min_value, max_value, voxel_size)] * 3)\n",
        "\n",
        "    # Define the implicit function for the torus using voxels.\n",
        "    voxels = (torch.sqrt(X ** 2 + Y ** 2) - R) ** 2 + Z ** 2 - r ** 2\n",
        "\n",
        "    # Extract vertices and faces using Marching Cubes.\n",
        "    vertices, faces = mcubes.marching_cubes(mcubes.smooth(voxels), isovalue=0)\n",
        "\n",
        "    # Convert vertices and faces to tensors.\n",
        "    vertices = torch.tensor(vertices).float()\n",
        "    faces = torch.tensor(faces.astype(int))\n",
        "\n",
        "    # Normalize vertex coordinates.\n",
        "    vertices = (vertices / voxel_size) * (max_value - min_value) + min_value\n",
        "\n",
        "    # Define textures based on vertex positions.\n",
        "    textures = (vertices - vertices.min()) / (vertices.max() - vertices.min())\n",
        "    textures = pytorch3d.renderer.TexturesVertex(textures.unsqueeze(0))\n",
        "\n",
        "    # Create a PyTorch3D Mesh representing the torus.\n",
        "    torus_mesh = pytorch3d.structures.Meshes(\n",
        "        [vertices],\n",
        "        [faces],\n",
        "        textures=textures\n",
        "    ).to(device)\n",
        "\n",
        "    # Define the view transformation using azimuth angle.\n",
        "    R, T = pytorch3d.renderer.look_at_view_transform(2, 3, azimuth, degrees=False)\n",
        "    cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
        "\n",
        "    # Get the mesh renderer and render the torus.\n",
        "    renderer = get_mesh_renderer(image_size=image_size, device=device)\n",
        "    rend = renderer(torus_mesh, cameras=cameras)\n",
        "\n",
        "    # Extract the RGB channels from the rendered image.\n",
        "    rend = rend[0, ..., :3]  # (B, H, W, 4) -> (H, W, 3)\n",
        "\n",
        "    return (rend.squeeze() * 255).byte().cpu().numpy()\n",
        "\n",
        "# Define a range of azimuth angles to create a 360-degree view.\n",
        "azimuth = torch.linspace(0, 2 * np.pi, 36)\n",
        "torus_views = []\n",
        "\n",
        "# Render the torus from different viewpoints and collect the images.\n",
        "for i in azimuth:\n",
        "    view = section5_3(i)\n",
        "    torus_views.append(view)\n",
        "\n",
        "# Generate a GIF for the 360-degree view.\n",
        "imageio.mimsave('/content/drive/MyDrive/assignment1-main/section5_3.gif', torus_views, duration=70, loop=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsqbgyK3WcxN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f466b2e04ec4aeaac68f2460cf6a45a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32705f6dd4ed4e709befb4c561c7c780": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77b88fb52c324eb6b926a3f0e82560dc",
              "IPY_MODEL_7dffa80b53314e0a8c731e19dad6574f",
              "IPY_MODEL_c7f548f8ead0463799985fe7d2f5a161"
            ],
            "layout": "IPY_MODEL_c90d4fec0af74f6f82d950331ca23cad"
          }
        },
        "6b1adbe457954bfdaef7eee5c80f900a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77b88fb52c324eb6b926a3f0e82560dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b1adbe457954bfdaef7eee5c80f900a",
            "placeholder": "​",
            "style": "IPY_MODEL_bd9fb3654fdf4589a1db21abd00fcad2",
            "value": "100%"
          }
        },
        "7dffa80b53314e0a8c731e19dad6574f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9db269b210544f9b526e33961a631f4",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7c777a2ef8846bb9c27ce0c030e871d",
            "value": 30
          }
        },
        "95a3bd58cf444ce0954386bc7812dda3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd9fb3654fdf4589a1db21abd00fcad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7f548f8ead0463799985fe7d2f5a161": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95a3bd58cf444ce0954386bc7812dda3",
            "placeholder": "​",
            "style": "IPY_MODEL_2f466b2e04ec4aeaac68f2460cf6a45a",
            "value": " 30/30 [00:00&lt;00:00, 75.57it/s]"
          }
        },
        "c90d4fec0af74f6f82d950331ca23cad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9db269b210544f9b526e33961a631f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7c777a2ef8846bb9c27ce0c030e871d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
